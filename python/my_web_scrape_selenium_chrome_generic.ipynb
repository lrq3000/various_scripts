{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium web scraper using chromium\n",
    "v0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Add the chromedriver to the path, by default in the current folder on the notebook in a subfolder webdriver, just place the binary inside\n",
    "os.environ['PATH'] += \";%swebdriver\" % (os.path.dirname(os.path.realpath(\"__file__\")) + '\\\\')\n",
    "os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "cookies = [{'name': 'cookie1', 'value': 'value1', 'domain': 'website.com'},\n",
    "          {'name': 'cookie2', 'value': 'value2', 'domain': 'website.com'}\n",
    "          ]  # cookies to get authenticated, can also use password via selenium IDE but this adds more steps and is less secure\n",
    "curpath = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "rootfolder = \"%s/%s\" % (curpath, 'downloaded')  # local base folder where to save to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Scraper at 0x1eb2b314730>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lxml import etree\n",
    "from pathvalidate import sanitize_filename\n",
    "from html2text import html2text\n",
    "import time\n",
    "import requests\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Generated by Selenium IDE\n",
    "import pytest\n",
    "import time\n",
    "import json\n",
    "#from selenium import webdriver\n",
    "from seleniumwire import webdriver  # this is NOT autogenerated, this allows to sniff media files\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "\n",
    "class Scraper():\n",
    "    def setup_method(self, method=None, cookies=None):\n",
    "        self.driver = webdriver.Chrome()\n",
    "        self.store = {}\n",
    "        self.store['base'] = \"https://www.website.com/\"\n",
    "        self.store['dummyurl'] = \"https://www.website.com/error404\"  # can also try to access an image instead\n",
    "        self.store['dummyurl_redirect'] = \"https://www.website.com/frontpage-after-login\"  # url to access after, because some websites require users to access the frontpage before being able to access sublinks as a protection against bots\n",
    "        self.store['realbase'] = \"https://www.website.com/listing-of-things-to-download\"\n",
    "        self.vars = {}\n",
    "\n",
    "    def preset_cookies(self, cookies=None):\n",
    "        \"\"\"Open a URL using the driver's base URL\"\"\"\n",
    "        # Navigate with cookies, need to open a dummy url on the same domain and then only can we open the true url we want\n",
    "        # From: https://stackoverflow.com/questions/36305660/selenium-js-add-cookie-to-request\n",
    "\n",
    "        #t.driver.delete_all_cookies()\n",
    "        #t.driver.get_cookies()\n",
    "\n",
    "        # Navigate to a dummy url on the same domain.\n",
    "        self.driver.get(self.store['dummyurl'])\n",
    "\n",
    "        # Load cookies\n",
    "        if cookies:\n",
    "            for c in cookies:\n",
    "                t.driver.add_cookie(c)\n",
    "\n",
    "        # # url to access after and wait a bit, otherwise other requests will fail, because some websites require users to access the frontpage before being able to access sublinks as a protection against bots\n",
    "        self.driver.get(self.store['dummyurl_redirect'])\n",
    "        time.sleep(3)\n",
    "\n",
    "    def count_links(self):\n",
    "        # Go to posters listing and scrape links\n",
    "        self.driver.get(self.store['realbase'])\n",
    "        all_links = self.driver.find_elements(By.XPATH, \"//ul[@id=\\'agenda\\']//a\")  # need to fetch all links everytime, otherwise they will become detached\n",
    "        # Count the number of links to make a loop\n",
    "        self.vars['all_links_count'] = len(all_links)\n",
    "\n",
    "    def scrape_all_abstracts(self, restart=None):\n",
    "        # Main loop to scrape everything\n",
    "        self.count_links()\n",
    "        for abstract_id in tqdm(range(self.vars['all_links_count'])):\n",
    "            if restart:\n",
    "                if abstract_id < restart:\n",
    "                    continue\n",
    "            # Go to posters listing and scrape links\n",
    "            self.driver.get(self.store['realbase'])\n",
    "            all_links = self.driver.find_elements(By.XPATH, \"//ul[@id=\\'agenda\\']//a\")  # need to fetch all links everytime, otherwise they will become detached\n",
    "            try:\n",
    "                all_links[abstract_id].click()\n",
    "            except:\n",
    "                continue  # skip to the next link if this one has issues\n",
    "            # Download files\n",
    "            self.download_abstract()\n",
    "            self.download_poster_mediafiles()\n",
    "            # Wait a random time to avoid bot being detected\n",
    "            random.uniform(1, 10)\n",
    "        # Done\n",
    "        return(1)\n",
    "\n",
    "    def scrape_one_abstract(self, abstract_id):\n",
    "        # Go to posters listing and scrape links\n",
    "        self.driver.get(self.store['realbase'])\n",
    "        all_links = self.driver.find_elements(By.XPATH, \"//ul[@id=\\'agenda\\']//a\")  # need to fetch all links everytime, otherwise they will become detached\n",
    "        try:\n",
    "            all_links[abstract_id].click()\n",
    "        except:\n",
    "            # skip to the next link if this one has issues\n",
    "            abstract_id += 1\n",
    "            all_links[abstract_id].click()\n",
    "        # Download files\n",
    "        self.download_abstract()\n",
    "        self.download_poster_mediafiles()\n",
    "        # Done\n",
    "        return(1)\n",
    "\n",
    "    def download_abstract(self):\n",
    "        # Download the abstract as HTML\n",
    "        # Wait a bit because otherwise the page may not have loaded yet\n",
    "        time.sleep(2)  # TODO: for more robust methods, see https://stackoverflow.com/questions/5868439/wait-for-page-load-in-selenium and https://artoftesting.com/wait-for-page-to-load-selenium-webdriver-java\n",
    "        # Get HTML source code\n",
    "        poster_abstract = self.driver.page_source\n",
    "        # Extract abstract title\n",
    "        tree = etree.HTML(poster_abstract)\n",
    "        r = tree.xpath('//h1')[0]\n",
    "        poster_title = r.text\n",
    "        # Create poster folder\n",
    "        poster_folder = \"%s/%s\" % (rootfolder, sanitize_filename(poster_title))\n",
    "        self.vars['poster_folder'] = poster_folder\n",
    "        if not os.path.exists(poster_folder):\n",
    "            os.makedirs(poster_folder)\n",
    "        # Save HTML in the adequate folder\n",
    "        with open(\"%s/abstract.html\" % (poster_folder), \"wb\") as f:\n",
    "            f.write(bytes(poster_abstract, encoding='utf-8'))\n",
    "        # Also save the abstract as a text (markdown) file, converting and removing all superfluous HTML markups\n",
    "        poster_abstract_body = tree.xpath('//div[@class=\\'main-popup-content\\']')\n",
    "        poster_abstract_body_html = etree.tostring(poster_abstract_body[0], pretty_print=True)\n",
    "        poster_abstract_body_text = html2text(str(poster_abstract_body_html)).replace('\\\\t', '').replace('\\\\n', '')[2:-1]\n",
    "        with open(\"%s/abstract.md\" % (poster_folder), \"wb\") as f:\n",
    "            f.write(bytes(poster_abstract_body_text, encoding='utf-8'))\n",
    "        # To download complete source code with CSS, JS etc:\n",
    "        # https://stackoverflow.com/questions/42900214/how-to-download-a-html-webpage-using-selenium-with-python\n",
    "        #poster_abstract  # debug\n",
    "        tree.xpath('//h1')[0].text\n",
    "\n",
    "    def download_poster_mediafiles(self):\n",
    "        # TODO: In the future, use Selenium if there are securities: https://sqa.stackexchange.com/questions/2197/how-to-download-a-file-using-seleniums-webdriver\n",
    "        # Clear previous requests, otherwise we will keep on redownloading the same stuff again and again\n",
    "        del self.driver.requests\n",
    "        # Access poster\n",
    "        self.driver.find_element(By.CSS_SELECTOR, \".pull-right > .btn-lg\").click()\n",
    "        # Sniff media files (audio, poster)\n",
    "        time.sleep(3)  # wait a bit for the poster to load, TODO: see driver.wait_for_request()\n",
    "        # Wait for the request/response to complete\n",
    "        self.driver.wait_for_request('4000px')\n",
    "        mediafiles = set([])\n",
    "        for request in self.driver.requests:\n",
    "            if request.response:\n",
    "                if '4000px.png' in request.url or request.response.headers['content-type'] == 'audio/mpeg':\n",
    "                    mediafiles.add(request.url)\n",
    "                    #print(\n",
    "                    #    request.url,\n",
    "                    #    request.response.status_code,\n",
    "                    #    request.response.headers['Content-Type']\n",
    "                    #)\n",
    "        # Download media files\n",
    "        poster_folder = self.vars['poster_folder']\n",
    "        for file_url in mediafiles:\n",
    "            file_dl = requests.get(file_url, allow_redirects=True)\n",
    "            filename = file_url.rsplit('/', 1)[1]\n",
    "            open('%s/%s' % (poster_folder, filename), 'wb').write(file_dl.content)\n",
    "        \n",
    "  \n",
    "    def teardown_method(self, method=None):\n",
    "        self.driver.quit()\n",
    "\n",
    "t = Scraper()\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.setup_method()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.preset_cookies(cookies=cookies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "if t.scrape_one_abstract(0) == 1:\n",
    "    print('All done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAUTION: to download ALL media files including audio and video,\n",
      "      you need to keep the browser window NOT minimized (ie, in background it's fine, but the window must be open),\n",
      "      otherwise the media won't play and thus won't be downloaded!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba2085d104c49afb0ba8844cc04a66b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1352.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "# Now for the real download of all links\n",
    "print('''CAUTION: to download ALL media files including audio and video,\n",
    "      you need to keep the browser window NOT minimized (ie, in background it's fine, but the window must be open),\n",
    "      otherwise the media won't play and thus won't be downloaded!''')\n",
    "if t.scrape_all_abstracts(restart=856) == 1:\n",
    "    print('All done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cookies = [{'name': 'cookie1', 'value': 'value1', 'domain': 'website.com'},\n",
    "          {'name': 'cookie2', 'value': 'value2', 'domain': 'website.com'}\n",
    "          ]\n",
    "url = \"https://website.com/frontpage-after-login\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url, cookies=cookies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if r.status_code != 200:\n",
    "    print('Failure to connect! Update cookies. Response code:')\n",
    "    print(r.status_code)\n",
    "\n",
    "r.text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
