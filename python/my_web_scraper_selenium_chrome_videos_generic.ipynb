{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium web scraper using chromium or chrome - for videos\n",
    "v0.4.2-final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Add the chromedriver to the path, by default in the current folder on the notebook in a subfolder webdriver, just place the binary inside\n",
    "# Download the exact version of the webdriver for your version of Chrome: https://chromedriver.chromium.org/downloads\n",
    "os.environ['PATH'] += \";%swebdriver\" % (os.path.dirname(os.path.realpath(\"__file__\")) + '\\\\')\n",
    "os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "cookies = [{'name': 'cookie1', 'value': 'value1', 'domain': 'website.com'},\n",
    "          {'name': 'cookie2', 'value': 'value2', 'domain': 'website.com'}\n",
    "          ]  # cookies to get authenticated, can also use password via selenium IDE but this adds more steps and is less secure\n",
    "# If this does not work or you don't get how to do that, simply log manually in the window that will open.\n",
    "#url = \"https://www.sleep2021.org/2021/SLEEP2021/PosterTitles.asp?PosterSortOrder=num&pfp=BrowsebyPosterID\"\n",
    "curpath = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "rootfolder = \"%s/%s\" % (curpath, 'downloaded')  # local base folder where to save to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from pathvalidate import sanitize_filename\n",
    "from html2text import html2text\n",
    "import time\n",
    "import requests\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Generated by Selenium IDE\n",
    "import pytest\n",
    "import time\n",
    "import json\n",
    "#from selenium import webdriver\n",
    "from seleniumwire import webdriver  # this is NOT autogenerated, this allows to sniff media files\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "\n",
    "class Scraper():\n",
    "    def setup_method(self, method=None, cookies=None):\n",
    "        self.driver = webdriver.Chrome()\n",
    "        self.store = {}\n",
    "        self.store['base'] = \"https://www.website.com/\"\n",
    "        self.store['dummyurl'] = \"https://www.website.com/error404\"  # can also try to access an image instead\n",
    "        self.store['dummyurl_redirect'] = \"https://www.website.com/frontpage-after-login\"  # url to access after, because some websites require users to access the frontpage before being able to access sublinks as a protection against bots\n",
    "        self.store['realbase'] = \"https://www.website.com/listing-of-things-to-download\"\n",
    "        self.vars = {}\n",
    "\n",
    "    def preset_cookies(self, cookies=None):\n",
    "        \"\"\"Open a URL using the driver's base URL\"\"\"\n",
    "        # Navigate with cookies, need to open a dummy url on the same domain and then only can we open the true url we want\n",
    "        # From: https://stackoverflow.com/questions/36305660/selenium-js-add-cookie-to-request\n",
    "\n",
    "        #t.driver.delete_all_cookies()\n",
    "        #t.driver.get_cookies()\n",
    "\n",
    "        # Navigate to a dummy url on the same domain.\n",
    "        self.driver.get(self.store['dummyurl'])\n",
    "\n",
    "        # Load cookies\n",
    "        if cookies:\n",
    "            for c in cookies:\n",
    "                t.driver.add_cookie(c)\n",
    "\n",
    "        # # url to access after and wait a bit, otherwise other requests will fail, because some websites require users to access the frontpage before being able to access sublinks as a protection against bots\n",
    "        self.driver.get(self.store['dummyurl_redirect'])\n",
    "        time.sleep(3)\n",
    "\n",
    "    def count_links(self):\n",
    "        # Go to posters listing and scrape links\n",
    "        self.driver.get(self.store['realbase'])\n",
    "        all_links = self.driver.find_elements(By.XPATH, \"//ul[@id=\\'agenda\\']//a\")  # need to fetch all links everytime, otherwise they will become detached\n",
    "        # Count the number of links to make a loop\n",
    "        self.vars['all_links_count'] = len(all_links)\n",
    "\n",
    "    def scrape_all_abstracts(self, restart=None):\n",
    "        # Main loop to scrape everything\n",
    "        self.count_links()\n",
    "        for abstract_id in tqdm(range(self.vars['all_links_count'])):\n",
    "            if restart:\n",
    "                if abstract_id < restart:\n",
    "                    continue\n",
    "            # Go to posters listing and scrape links\n",
    "            self.driver.get(self.store['realbase'])\n",
    "            all_links = self.driver.find_elements(By.XPATH, \"//ul[@id=\\'agenda\\']//a\")  # need to fetch all links everytime, otherwise they will become detached\n",
    "            try:\n",
    "                all_links[abstract_id].click()\n",
    "            except:\n",
    "                continue  # skip to the next link if this one has issues\n",
    "            # Download files\n",
    "            self.download_abstract()\n",
    "            self.download_poster_mediafiles()\n",
    "            # Wait a random time to avoid bot being detected\n",
    "            time.sleep(random.uniform(1, 10))\n",
    "        # Done\n",
    "        return(1)\n",
    "\n",
    "    def scrape_one_abstract(self, abstract_id):\n",
    "        # Go to posters listing and scrape links\n",
    "        self.driver.get(self.store['realbase'])\n",
    "        all_links = self.driver.find_elements(By.XPATH, \"//ul[@id=\\'agenda\\']//a\")  # need to fetch all links everytime, otherwise they will become detached\n",
    "        try:\n",
    "            all_links[abstract_id].click()\n",
    "        except:\n",
    "            # skip to the next link if this one has issues\n",
    "            abstract_id += 1\n",
    "            all_links[abstract_id].click()\n",
    "        # Download files\n",
    "        self.download_abstract()\n",
    "        self.download_poster_mediafiles()\n",
    "        # Done\n",
    "        return(1)\n",
    "\n",
    "    def download_abstract(self):\n",
    "        # Download the abstract as HTML\n",
    "        # Wait a bit because otherwise the page may not have loaded yet\n",
    "        time.sleep(2)  # TODO: for more robust methods, see https://stackoverflow.com/questions/5868439/wait-for-page-load-in-selenium and https://artoftesting.com/wait-for-page-to-load-selenium-webdriver-java\n",
    "        # Get HTML source code\n",
    "        poster_abstract = self.driver.page_source\n",
    "        # Extract abstract title\n",
    "        tree = etree.HTML(poster_abstract)\n",
    "        r = tree.xpath('//h1')[0]\n",
    "        poster_title = r.text\n",
    "        # Create poster folder\n",
    "        poster_folder = \"%s/%s\" % (rootfolder, sanitize_filename(poster_title))\n",
    "        self.vars['poster_folder'] = poster_folder\n",
    "        if not os.path.exists(poster_folder):\n",
    "            os.makedirs(poster_folder)\n",
    "        # Save HTML in the adequate folder\n",
    "        with open(\"%s/abstract.html\" % (poster_folder), \"wb\") as f:\n",
    "            f.write(bytes(poster_abstract, encoding='utf-8'))\n",
    "        # Also save the abstract as a text (markdown) file, converting and removing all superfluous HTML markups\n",
    "        poster_abstract_body = tree.xpath('//div[@class=\\'main-popup-content\\']')\n",
    "        poster_abstract_body_html = etree.tostring(poster_abstract_body[0], pretty_print=True)\n",
    "        poster_abstract_body_text = html2text(str(poster_abstract_body_html)).replace('\\\\t', '').replace('\\\\n', '')[2:-1]\n",
    "        with open(\"%s/abstract.md\" % (poster_folder), \"wb\") as f:\n",
    "            f.write(bytes(poster_abstract_body_text, encoding='utf-8'))\n",
    "        # To download complete source code with CSS, JS etc:\n",
    "        # https://stackoverflow.com/questions/42900214/how-to-download-a-html-webpage-using-selenium-with-python\n",
    "        #poster_abstract  # debug\n",
    "        tree.xpath('//h1')[0].text\n",
    "\n",
    "    def download_poster_mediafiles(self):\n",
    "        # TODO: In the future, use Selenium if there are securities: https://sqa.stackexchange.com/questions/2197/how-to-download-a-file-using-seleniums-webdriver\n",
    "        # Clear previous requests, otherwise we will keep on redownloading the same stuff again and again\n",
    "        del self.driver.requests\n",
    "        # Access poster\n",
    "        self.driver.find_element(By.CSS_SELECTOR, \".pull-right > .btn-lg\").click()\n",
    "        # Sniff media files (audio, poster)\n",
    "        time.sleep(3)  # wait a bit for the poster to load, TODO: see driver.wait_for_request()\n",
    "        # Wait for the request/response to complete\n",
    "        self.driver.wait_for_request('4000px')\n",
    "        mediafiles = set([])\n",
    "        for request in self.driver.requests:\n",
    "            if request.response:\n",
    "                if '4000px.png' in request.url or request.response.headers['content-type'] == 'audio/mpeg':\n",
    "                    mediafiles.add(request.url)\n",
    "                    #print(\n",
    "                    #    request.url,\n",
    "                    #    request.response.status_code,\n",
    "                    #    request.response.headers['Content-Type']\n",
    "                    #)\n",
    "        # Download media files\n",
    "        poster_folder = self.vars['poster_folder']\n",
    "        for file_url in mediafiles:\n",
    "            file_dl = requests.get(file_url, allow_redirects=True)\n",
    "            filename = file_url.rsplit('/', 1)[1]\n",
    "            open('%s/%s' % (poster_folder, filename), 'wb').write(file_dl.content)\n",
    "        \n",
    "  \n",
    "    def teardown_method(self, method=None):\n",
    "        self.driver.quit()\n",
    "\n",
    "t = Scraper()\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.setup_method()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.preset_cookies(cookies=cookies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sanitize_filename import sanitize\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# XPath can be tested with Chrome's developer's tool, in the Elements tab, in place of a search string\n",
    "t.store['realbase'] = \"https://www.website.com/listing-of-things-to-download\"\n",
    "t.driver.get(t.store['realbase'])\n",
    "all_links = t.driver.find_elements(By.XPATH, \"//ul[@id=\\'agenda\\']//li[contains(@class, \\'list-group-item\\')]/div[contains(@class, \\'prestitle\\')]/span[position()=1]\")  # need to fetch all links everytime, otherwise they will become detached\n",
    "print(len(all_links))\n",
    "for link_idx in tqdm(range(len(all_links))):\n",
    "    t.driver.get(t.store['realbase'])\n",
    "    all_links = t.driver.find_elements(By.XPATH, \"//ul[@id=\\'agenda\\']//li[contains(@class, \\'list-group-item\\')]/div[contains(@class, \\'prestitle\\')]/span[position()=1]\")  # need to fetch all links everytime, otherwise they will become detached\n",
    "    all_links[link_idx].click()\n",
    "    time.sleep(3)  # wait a bit for the list to show\n",
    "\n",
    "    h1 = sanitize(t.driver.find_elements(By.XPATH, r\"//h1\")[0].text)\n",
    "    all_sub_links = t.driver.find_elements(By.XPATH, r\"//div[contains(@class, 'current-card')]//li[contains(@class, 'loadbyurl')]\")\n",
    "    print(len(all_sub_links))\n",
    "    slink_idx = 0\n",
    "    for _ in tqdm(range(len(all_sub_links))):\n",
    "        t.driver.get(t.store['realbase'])\n",
    "        all_links = t.driver.find_elements(By.XPATH, \"//ul[@id=\\'agenda\\']//li[contains(@class, \\'list-group-item\\')]/div[contains(@class, \\'prestitle\\')]/span[position()=1]\")  # need to fetch all links everytime, otherwise they will become detached\n",
    "        all_links[link_idx].click()\n",
    "        time.sleep(3)  # wait a bit for the list to show\n",
    "        all_sub_links = t.driver.find_elements(By.XPATH, r\"//div[contains(@class, 'current-card')]//li[contains(@class, 'loadbyurl')]\")\n",
    "        try:\n",
    "            all_sub_links[slink_idx].click()\n",
    "            slink_idx += 1\n",
    "        except Exception as exc:\n",
    "            slink_idx += 1\n",
    "            print('Not interactable')\n",
    "            continue\n",
    "        time.sleep(2)\n",
    "\n",
    "        h1_2 = sanitize(t.driver.find_elements(By.XPATH, r\"//h1\")[0].text)\n",
    "        video_title = \"%s - %s\" % (h1, h1_2)\n",
    "        # Sanitize to make a valid filename\n",
    "        video_title = sanitize(video_title)  # remove forbidden characters\n",
    "        video_title = video_title[:150]  # limit if too long\n",
    "        print(video_title)\n",
    "        \n",
    "        try:\n",
    "            watch_button = t.driver.find_elements(By.XPATH, r\"//li[contains(@class, 'speakerrow')]//a[text()[contains(.,'Watch Now')]]\")[0]\n",
    "            #watch_button[0].click()\n",
    "            t.driver.get(watch_button.get_attribute('href'))\n",
    "            time.sleep(3)\n",
    "            subpath = \"%s/%s/%s\" % (rootfolder, h1, slink_idx)\n",
    "            os.makedirs(subpath)\n",
    "\n",
    "            # Download talks (split per slide)\n",
    "            videofiles = re.findall(r'https:[^\"]+mp4[^\"]+', t.driver.page_source, re.I)[1:]  # the first is the one playing, it's the same as the second item which is the first in the JS list\n",
    "            for idx, file_url in enumerate(videofiles):\n",
    "                file_url = file_url.replace('\\\\', '')  # unescape (dirtily)\n",
    "                file_dl = requests.get(file_url, allow_redirects=True)\n",
    "                filename = \"%s Slide%i.mp4\" % (video_title, idx)\n",
    "                open('%s/%s' % (subpath, filename), 'wb').write(file_dl.content)\n",
    "\n",
    "            # Download slides\n",
    "            pngfiles = re.findall(r'https:\\\\[^\"]+(?:png|jpg|jpeg)[^\"]+', t.driver.page_source, re.I)  # the first is the one playing, it's the same as the second item which is the first in the JS list\n",
    "            for idx, file_url in enumerate(pngfiles):\n",
    "                #print(file_url)\n",
    "                file_url = file_url.replace('\\\\', '')  # unescape (dirtily)\n",
    "                file_dl = requests.get(file_url, allow_redirects=True)\n",
    "                filename = \"%s Slide%i.png\" % (video_title, idx)\n",
    "                open('%s/%s' % (subpath, filename), 'wb').write(file_dl.content)\n",
    "        except Exception as exc:\n",
    "                print(exc)\n",
    "                continue\n",
    "print('All done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cookies = {'14014': 'AccountKey=0034N00003AicFgQAJ',\n",
    "          'ASPSESSIONIDCEBCRRRD': 'KEDCALBCPCELLBONDDOKBNDH',\n",
    "          'ASPSESSIONIDQACCQDTR': 'NIKLFFKCKAKGEEOPGJJBIGHC',\n",
    "          'ASPSESSIONIDQECCQDTR': 'MIKLFFKCCPPLDPCEIJDHIAJC'}\n",
    "url = \"https://www.sleep2021.org/2021/SLEEP2021/PosterTitles.asp?PosterSortOrder=num&pfp=BrowsebyPosterID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url, cookies=cookies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if r.status_code != 200:\n",
    "    print('Failure to connect! Update cookies. Response code:')\n",
    "    print(r.status_code)\n",
    "\n",
    "r.text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
